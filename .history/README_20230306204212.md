# KnowledgeLifecycle
This is the paper list of survey paper "The Life Cycle of Knowledge in Big Language Models: A Survey".
We will update the survey content and this paper list regularly, and we very much welcome suggestions of any kind.

![image](https://github.com/c-box/KnowledgeLifecycle/blob/main/Figures/lifecircle.png) 

- [KnowledgeLifecycle](#knowledgelifecycle)
  * [Abstract](#abstract)
  * [1. Knowledge Acquisition](#1-knowledge-acquisition)
    + [1.1 Learning from Text Data](#11-learning-from-text-data)

      

    + [1.2 Learning from Structured Data](#12-learning-from-structured-data)
  * [2. Knowledge Representation](#2-knowledge-representation)
    + [2.1 Gradient-based](#21-gradient-based)
    + [2.2 Causal-inspired](#22-causal-inspired)
    + [2.3 Attention-based](#23-attention-based)
    + [2.4 Layer-wise](#24-layer-wise)
  * [3. Knowledge Probing](#3-knowledge-probing)
    + [3.1 Prompt-based Probing](#31-prompt-based-probing)
    + [3.2 Feature-based Probing](#32-feature-based-probing)
  * [4. Knowledge Editing](#4-knowledge-editing)
    + [4.1 Constrained Fine-tuning](#41-constrained-fine-tuning)
    + [4.2 Memory-based](#42-memory-based)
    + [4.3 Meta-Learning](#43-meta-learning)
    + [4.4 Locate and Edit](#44-locate-and-edit)
  * [5. Knowledge Application](#5-knowledge-application)
    + [5.1 Language Models as Knowledge Bases](#51-language-models-as-knowledge-bases)
    + [5.2 Language Models for Downstream Tasks](#52-language-models-for-downstream-tasks)
      - [5.2.1 Fine-tuning](#521-fine-tuning)
      - [5.2.2 Prompt-tuning](#522-prompt-tuning)
      - [5.2.3 In-context Learning](#523-in-context-learning)
  * [Reference](#reference)

## Abstract
Knowledge plays a critical role in artificial intelligence. 
Recently, the extensive success of pre-trained language models (PLMs) has raised significant attention about how knowledge can be acquired, maintained, updated and used by language models. Despite the enormous amount of related studies, there still lacks a unified view of how knowledge circulates within language models throughout the learning, tuning, and application processes, which may prevent us from further understanding the connections between current progress or realizing existing limitations. 
**In this survey, we revisit PLMs as knowledge-based systems by dividing the life circle of knowledge in PLMs into five critical periods, and investigating how knowledge circulates when it is built, maintained and used**. To this end, we systematically review existing studies of each period of the knowledge life cycle, summarize the main challenges and current limitations, and discuss future directions.

## 1. Knowledge Acquisition

### 1.1. Learning from Text Data

Language models are unsupervised multitask learners. (OpenAI blog 2019)

Language Models are Few-Shot Learners. (NIPS 2020) [[paper](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)]

Training language models to follow instructions with human feedback. (2022) [[paper](https://arxiv.org/abs/2203.02155)]

Bloom: A 176b-parameter open-access multilingual language model. (2022) [[paper](https://arxiv.org/abs/2211.05100)]

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. (NAACL 2019) [[paper](https://aclanthology.org/N19-1423)]

Roberta: A robustly optimized bert pretraining approach. (2019) [[paper](https://arxiv.org/abs/1907.11692)]

Exploring the limits of transfer learning with a unified text-to-text transformer.. (J. Mach. Learn. Res. 2020)

MASS: Masked Sequence to Sequence Pre-training for Language Generation. (Proc. of ICML 2019) [[paper](http://proceedings.mlr.press/v97/song19d.html)]

BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. (ACL 2020) [[paper](https://aclanthology.org/2020.acl-main.703)]

Pretrained Language Model Embryology: The Birth of ALBERT. (EMNLP 2020) [[paper](https://aclanthology.org/2020.emnlp-main.553)]

ALBERT: A Lite BERT for Self-supervised Learning of Language
Representations. (ICLR 2020) [[paper](https://openreview.net/forum?id=H1eA7AEtvS)]

How much pretraining data do language models need to learn syntax?. (EMNLP 2021) [[paper](https://aclanthology.org/2021.emnlp-main.118)]

Roberta: A robustly optimized bert pretraining approach. (2019) [[paper](https://arxiv.org/abs/1907.11692)]

Probing Across Time: What Does RoBERTa Know and When?. (Findings of ACL 2021) [[paper](https://aclanthology.org/2021.findings-emnlp.71)]

### 1.2. Learning from Structured Data

Ernie: Enhanced representation through knowledge integration. (2019) [[paper](https://arxiv.org/abs/1904.09223)]

Exploiting Structured Knowledge in Text via Graph-Guided Representation Learning. (EMNLP 2020) [[paper](https://aclanthology.org/2020.emnlp-main.722)]

Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language
Model. (ICLR 2020) [[paper](https://openreview.net/forum?id=BJlzm64tDH)]

LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention. (EMNLP 2020) [[paper](https://aclanthology.org/2020.emnlp-main.523)]

Entities as Experts: Sparse Memory Access with Entity Supervision. (EMNLP 2020) [[paper](https://aclanthology.org/2020.emnlp-main.400)]

Zero-Shot Entity Linking by Reading Entity Descriptions. (ACL 2019) [[paper](https://aclanthology.org/P19-1335)]

Learning Dense Representations for Entity Retrieval. (Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL) 2019) [[paper](https://aclanthology.org/K19-1049)]

Knowledge Enhanced Contextual Word Representations. (EMNLP 2019) [[paper](https://aclanthology.org/D19-1005)]

LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention. (EMNLP 2020) [[paper](https://aclanthology.org/2020.emnlp-main.523)]

ERNIE: Enhanced Language Representation with Informative Entities. (ACL 2019) [[paper](https://aclanthology.org/P19-1139)]

KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation. (TACL 2021) [[paper](https://aclanthology.org/2021.tacl-1.11)]

K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters. (Findings of ACL 2021) [[paper](https://aclanthology.org/2021.findings-acl.121)]

ERICA: Improving Entity and Relation Understanding for Pre-trained Language Models via Contrastive Learning. (ACL 2021) [[paper](https://aclanthology.org/2021.acl-long.260)]

Self-Supervised Knowledge Triplet Learning for Zero-Shot Question Answering. (EMNLP 2020) [[paper](https://aclanthology.org/2020.emnlp-main.11)]

K-BERT: Enabling Language Representation with Knowledge Graph. (AAAI 2020) [[paper](https://aaai.org/ojs/index.php/AAAI/article/view/5681)]

Matching the Blanks: Distributional Similarity for Relation Learning. (ACL 2019) [[paper](https://aclanthology.org/P19-1279)]

COMET: Commonsense Transformers for Automatic Knowledge Graph Construction. (ACL 2019) [[paper](https://aclanthology.org/P19-1470)]

A Knowledge-Enhanced Pretraining Model for Commonsense Story Generation. (TACL 2020) [[paper](https://aclanthology.org/2020.tacl-1.7)]

Unsupervised Commonsense Question Answering with Self-Talk. (EMNLP 2020) [[paper](https://aclanthology.org/2020.emnlp-main.373)]

Align, mask and select: A simple method for incorporating commonsense knowledge into language representation models. (2019) [[paper](https://arxiv.org/abs/1908.06725)]

Knowledge-driven data construction for zero-shot evaluation in commonsense question answering. (AAAI 2021)

SentiLARE: Sentiment-Aware Language Representation Learning with Linguistic Knowledge. (EMNLP 2020) [[paper](https://aclanthology.org/2020.emnlp-main.567)]

SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis. (ACL 2020) [[paper](https://aclanthology.org/2020.acl-main.374)]

Specializing Unsupervised Pretraining Models for Word-Level Semantic Similarity. (COLING 2020) [[paper](https://aclanthology.org/2020.coling-main.118)]

SenseBERT: Driving Some Sense into BERT. (ACL 2020) [[paper](https://aclanthology.org/2020.acl-main.423)]

Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (2022) [[paper](https://arxiv.org/abs/2210.08471)]

Limit-bert: Linguistic informed multi-task bert. (2019) [[paper](https://arxiv.org/abs/1910.14296)]

Do Syntax Trees Help Pre-trained Transformers Extract Information?. (EACL 2021) [[paper](https://aclanthology.org/2021.eacl-main.228)]

Syntax-BERT: Improving Pre-trained Transformers with Syntax Trees. (EACL 2021) [[paper](https://aclanthology.org/2021.eacl-main.262)]

## 2. Knowledge Representation

### 2.1. Gradient-based

Knowledge Neurons in Pretrained Transformers. (ACL 2022) [[paper](https://aclanthology.org/2022.acl-long.581)]

Transformer Feed-Forward Layers Are Key-Value Memories. (EMNLP 2021) [[paper](https://aclanthology.org/2021.emnlp-main.446)]

### 2.2. Causal-inspired

Locating and editing factual knowledge in gpt. (2022) [[paper](https://arxiv.org/abs/2202.05262)]

### 2.3. Attention-based

What Does BERT Look at? An Analysis of BERT's Attention. (ACL Workshop BlackboxNLP 2019) [[paper](https://aclanthology.org/W19-4828)]

Do Attention Heads in BERT Track Syntactic Dependencies?. (2019) [[paper](https://arxiv.org/abs/1911.12246)]

Open Sesame: Getting inside BERT's Linguistic Knowledge. (ACL Workshop BlackboxNLP 2019) [[paper](https://aclanthology.org/W19-4825)]

### 2.4. Layer-wise

Open Sesame: Getting inside BERT's Linguistic Knowledge. (ACL Workshop BlackboxNLP 2019) [[paper](https://aclanthology.org/W19-4825)]

Linguistic Knowledge and Transferability of Contextual Representations. (NAACL 2019) [[paper](https://aclanthology.org/N19-1112)]

BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (BlackboxNLP Workshop 2020) [[paper](https://aclanthology.org/2020.blackboxnlp-1.17)]

Finding patterns in Knowledge Attribution for Transformers. (2022) [[paper](https://arxiv.org/abs/2205.01366)]

## 3. Knowledge Probing

### 3.1 Prompt-based Probing

### 3.2 Feature-based Probing

## 4. Knowledge Editing

### 4.1 Constrained Fine-tuning

### 4.2 Memory-based

### 4.3 Meta-Learning

### 4.4 Locate and Edit

## 5. Knowledge Application

### 5.1 Language Models as Knowledge Bases

### 5.2 Language Models for Downstream Tasks

#### 5.2.1. Fine-tuning

#### 5.2.2. Prompt-tuning

#### 5.2.3. In-context Learning


## Reference
If this repository helps you, please kindly cite the following bibtext:
```
To be updated
```

